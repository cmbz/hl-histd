{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curation: Trade Statistics Data Curation Pilot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "- This script uploads content in the inventory of files associated with the volume: `Trade statistics of the treaty ports, for the period 1863-1872` to `demo.dataverse.org`. The curation approach creates one dataset per series name.\n",
    "- **Created:** 2023/03/28\n",
    "- **Updated:** 2023/04/03"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals\n",
    "- Global variables for this script. \n",
    "- Set variable names (e.g., `g_api_key` as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set curation source path\n",
    "g_module_path = './'\n",
    "\n",
    "# path to output file\n",
    "g_dataverse_inventory_file = './trade_statistics_inventory.csv'\n",
    "\n",
    "# series names\n",
    "g_series_names = []\n",
    "\n",
    "# dataset inventories (keyed on series name)\n",
    "g_series_inventories = {}\n",
    "\n",
    "# dataset metadata (keyed on series name)\n",
    "g_dataset_metadata = {}\n",
    "\n",
    "# dataverse installation\n",
    "g_dataverse_installation_url = 'https://demo.dataverse.org'\n",
    "\n",
    "# dataverse API key\n",
    "g_dataverse_api_key = 'xxxxxx'\n",
    "\n",
    "# dataverse collection name\n",
    "g_dataverse_collection = 'trade_statistics'\n",
    "\n",
    "# dataverse inventory dataframe\n",
    "g_dataverse_inventory_df = None\n",
    "\n",
    "# dataset author\n",
    "g_dataset_author = 'Last, First'\n",
    "\n",
    "# dataset author affiliation\n",
    "g_dataset_author_affiliation = 'Harvard Library'\n",
    "\n",
    "# dataset contact information\n",
    "g_dataset_contact = 'Last, First'\n",
    "g_dataset_contact_email = 'last_first@harvard.edu'\n",
    "\n",
    "# full path to location of datafiles (e.g., ../data/trade_statistics)\n",
    "g_datafiles_path = 'xxxxxxxx'\n",
    "\n",
    "# demo dataverse dataset information (keyed on series name)\n",
    "g_dataverse_dataset_info = {}\n",
    "\n",
    "# datafile metadata (dataframe of datafile metadata, keyed on series name)\n",
    "g_datafile_metadata = {}\n",
    "\n",
    "# datafile description template\n",
    "g_datafile_description_template = 'File associated with data tables series:'\n",
    "\n",
    "# dataset batches (array of batches of series to create/upload)\n",
    "g_dataset_batches = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add local modules path to Jupyter system path\n",
    "- Load all modules including local modules such as `curate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if g_module_path not in sys.path:\n",
    "    sys.path.append(g_module_path)\n",
    "\n",
    "import curate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pprint\n",
    "from pyDataverse.api import NativeApi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dictionary of dataset pids keyed on series name\n",
    "def get_dataset_pids(batch, dataset_info):\n",
    "    pids = {}\n",
    "    for series_name in batch:\n",
    "        pids[series_name] = dataset_info[series_name].get('dataset_pid')\n",
    "    return pids\n",
    "\n",
    "# get dictionary of datafile inventories keyed on series name\n",
    "def get_datafile_inventories(batch, datafile_metadata):\n",
    "    inventories = {}\n",
    "    for series_name in batch:\n",
    "        inventories[series_name] = datafile_metadata[series_name]\n",
    "    return inventories \n",
    "\n",
    "# upload the datafiles associated with a batch\n",
    "def upload_dataset_batch(api, dataverse_url, batch_list, batch_pids, batch_datafile_metadata, data_directory):\n",
    "    # upload the datafiles associated with each series in the batch\n",
    "    results = {}\n",
    "    for series_name in batch_list:\n",
    "        pid = batch_pids[series_name]\n",
    "        datafiles_metadata = batch_datafile_metadata[series_name]\n",
    "        results[series_name] = curate.direct_upload_datafiles(api, dataverse_url, pid, data_directory, datafiles_metadata)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate Inventory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare inventory data for curation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Read `dataverse_inventory`\n",
    "- Create a `DataFrame` for later use\n",
    "- Note: It was necessary to delete `csv` entries with `table_type` = Missing because those files did not appear in the inventory \n",
    "- Note: Two additional file paths were removed from the inventory: `005825557_pt1_00118.innodata.csv` and `005825557_pt2_00127.innodata.csv`. Although these two files appeared in the METS file, they did not appear in the original files using DRS ids as names, and therefore were not renamed using the owner-supplied naming scheme.\n",
    "- Note: Also, the `curate:direct_upload_datafiles` function expects all files to be in a single directory (not grouped by file type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataverse inventory file\n",
    "g_dataverse_inventory_df = pd.read_csv(g_dataverse_inventory_file,index_col=None,low_memory=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Create Dataset Inventories\n",
    "- Get the list of series names\n",
    "- Create a `dict` of file inventories keyed on series name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of series in the full inventory\n",
    "g_series_names = list(g_dataverse_inventory_df.series_name.unique())\n",
    "\n",
    "# create series inventories\n",
    "for name in g_series_names:\n",
    "    # get series inventory\n",
    "    g_series_inventories[name] = g_dataverse_inventory_df.loc[g_dataverse_inventory_df['series_name'] == name]\n",
    "\n",
    "pprint.pprint(g_series_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Create Dataset Metadata\n",
    "- Create a `dict` of dataset metadata extracted from each inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each series name, create dataset metadata\n",
    "for series_name in g_series_names:\n",
    "    # get series inventory\n",
    "    series_inventory = g_series_inventories[series_name]\n",
    "    md = curate.create_dataset_metadata(g_dataset_author, g_dataset_author_affiliation, \n",
    "                                        g_dataset_contact, g_dataset_contact_email,\n",
    "                                        series_name, series_inventory)\n",
    "    g_dataset_metadata[series_name] = md\n",
    "\n",
    "pprint.pprint(g_dataset_metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create Datafile Metadata\n",
    "- Create a `dict` of `DataFrames` containing metadata about individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for series_name in g_series_names:\n",
    "    # get dataset metadata for the series\n",
    "    series_metadata = g_dataset_metadata[series_name]\n",
    "    # get the series inventory\n",
    "    series_inventory_df = g_series_inventories[series_name]\n",
    "    # create datafile metadata\n",
    "    g_datafile_metadata[series_name] = curate.create_datafile_metadata(series_inventory_df, g_datafile_description_template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create Series Batches\n",
    "- Create a set of (approximately) equal length batches of series (to create dataset and upload datafiles)\n",
    "- Generally, there are too many series in a volume to create the related datasets and then upload all their datafiles in a single tight loop. Therefore, it's useful to create batches of these series and perform the create/upload operation on a single batch at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of series in a batch\n",
    "batch_size = 5\n",
    "g_batches = np.array_split(g_series_names, len(g_series_names)/batch_size)\n",
    "\n",
    "pprint.pprint(g_batches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize `pyDataverse` API\n",
    "- Use `pyDataverse` to initialize the API to the dataverse installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pyDataverse API adapter\n",
    "g_api = NativeApi(g_dataverse_installation_url, g_dataverse_api_key)\n",
    "\n",
    "# print results\n",
    "print('{}'.format(g_api))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Datasets and Upload Datafiles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create all datasets\n",
    "- For each series name, create a dataset and retain status information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each series, create a dataset and save its information\n",
    "for series_name in g_series_names:\n",
    "    # get the series metadata\n",
    "    series_metadata = g_dataset_metadata[series_name]\n",
    "    # create the dataset\n",
    "    g_dataverse_dataset_info[series_name] = curate.create_dataset(g_api, g_dataverse_collection, series_metadata)\n",
    "\n",
    "pprint.pprint(g_dataverse_dataset_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Upload dataset datafiles, one batch at a time\n",
    "- Upload the datafiles associated with each dataset in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 0\n",
    "index = 0\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)\n",
    "pprint.pprint(pids)\n",
    "pprint.pprint(datafile_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 1\n",
    "index = 1\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 2\n",
    "index = 2\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 3\n",
    "index = 3\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 4\n",
    "index = 4\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 5\n",
    "index = 5\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 6\n",
    "index = 6\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 7\n",
    "index = 7\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 8\n",
    "index = 8\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 9\n",
    "index = 9\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 10\n",
    "index = 10\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 11\n",
    "index = 11\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 12\n",
    "index = 12\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 13\n",
    "index = 13\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 14\n",
    "index = 14\n",
    "batch = g_batches[index]\n",
    "pids = get_dataset_pids(batch, g_dataverse_dataset_info)\n",
    "datafile_metadata = get_datafile_inventories(batch, g_datafile_metadata)\n",
    "print('Uploading batch: {}, series: {}'.format(index, batch))\n",
    "errors = upload_dataset_batch(g_api, g_dataverse_installation_url, \n",
    "                              batch, pids, datafile_metadata, g_datafiles_path)\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "import importlib\n",
    "importlib.reload(curate)\n",
    "\n",
    "# publish the datasets\n",
    "errors = curate.publish_datasets(g_api, g_dataverse_collection, version='major')\n",
    "\n",
    "pprint.pprint(errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Curation Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Create a single dataset\n",
    "This test allows users to create a single dataset and upload its related datafiles. \n",
    "Useful for troubleshooting and to test other collections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Test: Create datafile metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datafile metadata\n",
    "# get the first series\n",
    "first_series = g_series_names[0]\n",
    "first_series_metadata = g_dataset_metadata[first_series]\n",
    "first_series_inventory_df = g_series_inventories[first_series]\n",
    "\n",
    "# set the template\n",
    "template = 'File associated with data tables series:'\n",
    "datafile_metadata_df = curate.create_datafile_metadata(first_series_inventory_df, template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Test: Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "dataset_ret = curate.create_dataset(g_api, g_dataverse_collection, first_series_metadata)\n",
    "pprint.pprint(dataset_ret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test: Direct upload the datafiles associated with the dataset (series name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the series dataset datafiles \n",
    "pid = dataset_ret.get('dataset_pid')\n",
    "ret = curate.direct_upload_datafiles(g_api, g_dataverse_installation_url, pid, g_datafiles_path, datafile_metadata_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Test: Examine a directory to make certain all files exist before attempting an upload of datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if all files are there and report the ones that aren't\n",
    "\n",
    "import os\n",
    "errors = {}\n",
    "for row in g_dataverse_inventory_df.iterrows():\n",
    "    filename = row[1].get('filename_osn')\n",
    "    filepath = g_datafiles_path + '/' + filename\n",
    "    if (os.path.exists(filepath)):\n",
    "        errors[filepath] = True\n",
    "    else:\n",
    "        print('File not found: {}'.format(filepath))\n",
    "        errors[filepath] = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Test: Delete all the datasets in the collection and start again\n",
    "- WARNING: This is a permanent operation. Be very certain you want to perform this operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all the datasets\n",
    "# ARE YOU SURE ABOUT THIS? if so, uncomment the next line and execute\n",
    "#ret = curate.delete_datasets(g_api, g_dataverse_collection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End document.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e80866da39f614c41262712a96c603cec09e65c25ffba1b64ff6a9fa5a13fe2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
