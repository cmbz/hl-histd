{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata: Prepare Dataverse Inventory\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "- This script prepares the volume: `Trade statistics of the treaty ports, for the period 1863-1872` for manual metadata application. \n",
    "    - **HOLLIS:** https://id.lib.harvard.edu/alma/990058255570203941/catalog\n",
    "    - **DRS:** https://iiif.lib.harvard.edu/manifests/view/drs:44319007$1i\n",
    "- Note that this script is tailored specifically for use with the Trade Statistics volume and is not suitable for generalization.\n",
    "- **Created:** 2023/03/20\n",
    "- **Updated:** 2023/03/28"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to output file\n",
    "g_output_file = '../tmp/dataverse_inventory.csv'\n",
    "\n",
    "# gather some information keyed on DRS id when we the various dataframes\n",
    "g_drs_urls = {}\n",
    "g_drs_image_osn = {}\n",
    "g_drs_csv_osn = {}\n",
    "g_drs_txt_osn = {}\n",
    "g_drs_alto_osn = {}\n",
    "g_drs_handwriting = {}\n",
    "g_drs_two_page = {}\n",
    "g_drs_table_type = {}\n",
    "g_drs_table_title = {}\n",
    "g_drs_multilevel_columns = {}\n",
    "g_drs_multilevel_rows = {}\n",
    "g_drs_computation_ready = {}\n",
    "g_drs_tablegroup = {}\n",
    "g_drs_tablegroup_members = {}\n",
    "g_drs_csv_size = {}\n",
    "g_drs_csv_shape = {}\n",
    "g_drs_csv_columns = {}\n",
    "g_drs_series_type = {}\n",
    "g_drs_series_num = {}\n",
    "\n",
    "# metadata associated with the entire trade statistics volume\n",
    "g_volume_metadata = {\n",
    "    'title':'Trade statistics of the treaty ports, for the period 1863-1872',\n",
    "    'attribution':'Compiled for the Austro-Hungarian Universal Exhibition, Vienna, 1873 to illustrate the international exchange of products ; published by order of the Inspector General of Chinese Maritime Customs.',\n",
    "    'author':'China. Hai guan zong shui wu si shu.',\n",
    "    'published':'Shanghai : Imperial Maritime Customs, 1873.',\n",
    "    'notes':'Includes separately paged documents entitled Statistics of trade, with separate title page, for the following ports: Newchwang, Tientsin, Chefoo, Hankow, Kiukiang, Chinkiang, Shanghai, Ningpo, Foochow, Tamsui, Takow, Amoy, Swatow, Canton.; Robert Hart, Inspector General.; Tables include: I. Tonnage.--II. Values.--III. Articles.--IV. Revenue.--V. Population.',\n",
    "    'subjects':['Vienna International Exhibition (1873)',\n",
    "                'China -- Catalogs','Harbors -- China',\n",
    "                'China -- Population -- Statistics',\n",
    "                'China -- Commerce -- Statistics'],\n",
    "    'creation_date':'1873',\n",
    "    'record_id':'990058255570203000',\n",
    "    'permalink':'https://hollis.harvard.edu/permalink/f/hg18ek/01HVD_ALMA211970791270003941'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pprint as pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read content and set up dataverse inventory `DataFrame` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# load required files\n",
    "#\n",
    "\n",
    "# load full vendor inventory\n",
    "vendor_inventory_df = pd.read_csv('../tmp/mapped_vendor_inventory.csv',index_col=None)\n",
    "\n",
    "# load manually reviewed qc file \n",
    "manual_qc_df = pd.read_csv('../tmp/manual_file_qc.csv',index_col=None)\n",
    "\n",
    "# load manually updated metadata file (metadata about csv files)\n",
    "manual_metadata_df = pd.read_csv('../tmp/manual_metadata.csv',index_col=None)\n",
    "manual_metadata_df.fillna('',inplace=True)\n",
    "\n",
    "# load lookup table relating DRS ids, IIIF urls, and images\n",
    "do_inventory_df = pd.read_csv('../tmp/do_inventory.csv')\n",
    "\n",
    "# load NLP entities\n",
    "nlp_entities_df = pd.read_csv('../tmp/nlp_entities.csv')\n",
    "\n",
    "#\n",
    "# create new inventory dataframe\n",
    "#\n",
    "\n",
    "# define columns\n",
    "columns = ['drs_id','url','filename_osn', 'filepath_osn', 'file_type',\n",
    "           'table_title','table_type','multilevel_columns','multilevel_rows','computation_ready',\n",
    "           'table_group','table_group_members','shape','size','columns','series_type','series_num','series_name',\n",
    "           'image_handwriting','image_two_page','related_image','related_csv','related_txt','related_alto','entities']\n",
    "\n",
    "# scope notes for the new columns\n",
    "column_scope_notes = {\n",
    "    'drs_id':'DRS id associated with the file',\n",
    "    'url':'IIIF url to resource in DRS',\n",
    "    'filename_osn':'Name of file mapped to owner-supplied name',\n",
    "    'filepath_osn':'Full path to file using owner-supplied name',\n",
    "    'file_type':'One of: image, txt, alto, or csv',\n",
    "    'table_title':'Title of the csv table, if file_type = csv',\n",
    "    'table_type':'One of Categorical, Comparison, Empty, Summary, Missing, or Other',\n",
    "    'multilevel_columns':'True or False',\n",
    "    'multilevel_rows':'True or False',\n",
    "    'computation_ready':'True or False',\n",
    "    'table_group':'True or False',\n",
    "    'table_group_members':'List of filenames of related tables, if any',\n",
    "    'shape':'Array: [width, height]',\n",
    "    'size':'Total number of cells in the table, width x height',\n",
    "    'columns':'List of column names, if available',\n",
    "    'series_type':'Type of series, e.g., NA, Tonnage, Values, or Article',\n",
    "    'series_num':'Number of the specific series',\n",
    "    'series_name':'Name of the series (series_type + series_num)',\n",
    "    'image_handwriting':'True or False',\n",
    "    'image_two_page':'True or False',\n",
    "    'related_image':'Filename for related image',\n",
    "    'related_csv':'CSV file/table related to this file, if any',\n",
    "    'related_txt':'Filename for related text',\n",
    "    'related_alto':'Filename for related alto file',\n",
    "    'entities':'NLP entities associated with the file, if any'\n",
    "}\n",
    "\n",
    "# create new dataframe with specififed columns\n",
    "dataverse_inventory_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate dataverse inventory `DataFrame`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect information from the do_inventory `DataFrame`\n",
    "- Note: the `do_inventory` is extracted from the digital object's IIIF manifest\n",
    "- Here, I collect the urls associated with each image in the digital object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the do_inventory_df\n",
    "for row in do_inventory_df.iterrows():\n",
    "    # collect information from dataframe\n",
    "    drs_id = row[1].get('drs_id')\n",
    "    url = row[1].get('url')\n",
    "    g_drs_urls[drs_id] = url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect information from the vendor_inventory `DataFrame`\n",
    "- Here, I collect the `filename_osn` and `filepath_osn` for each file in the inventory and group them by `file_type` (e.g. image, csv, alto, txt). These elements will be used later on when assigning `related_image`, `related_csv`, and `related_tx`t` fields in the final dataverse inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the vendor_inventory_df\n",
    "for row in vendor_inventory_df.iterrows():\n",
    "    # collect information from dataframe\n",
    "    drs_id = row[1].get('drs_id')\n",
    "    file_type = row[1].get('file_type')\n",
    "    filename_osn = row[1].get('filename_osn')\n",
    "    filepath_osn = row[1].get('filepath_osn')\n",
    "    if (file_type == 'alto'):\n",
    "        g_drs_alto_osn[drs_id] = filename_osn\n",
    "    elif (file_type == 'csv'): \n",
    "        g_drs_csv_osn[drs_id] = filename_osn\n",
    "    elif (file_type == 'image'): \n",
    "        g_drs_image_osn[drs_id] = filename_osn\n",
    "    elif (file_type == 'txt'): \n",
    "        g_drs_txt_osn[drs_id] = filename_osn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect information based upon the DRS ID from the manual_qc inventory\n",
    "- The `manual_qc_df` contains information about which images feature two-page tables (`g_drs_two_page`) and which feature handwriting (`g_drs_handwriting`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the manual_qc_df\n",
    "for row in manual_qc_df.iterrows():\n",
    "    # collect variables\n",
    "    drs_id = row[1].get('drs_id')\n",
    "    note = row[1].get('note')\n",
    "    # set defaults\n",
    "    g_drs_two_page[drs_id] = False\n",
    "    g_drs_handwriting[drs_id] = False\n",
    "    # set values\n",
    "    if note == '2-page tables':\n",
    "        g_drs_two_page[drs_id] = True\n",
    "        two_page = True\n",
    "    elif note == 'handwriting':\n",
    "        g_drs_handwriting[drs_id] = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection information based upon the DRS ID from the manual_metadata inventory\n",
    "- The `manual_metadata_df` contains information about tables that have been manually assigned, such as the `g_drs_table_type` and `g_drs_tablegroups`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the manual_metadata_df\n",
    "for row in manual_metadata_df.iterrows():\n",
    "    # collect variables\n",
    "    drs_id = row[1].get('drs_id')\n",
    "    g_drs_table_type[drs_id] = row[1].get('table_type')\n",
    "    g_drs_table_title[drs_id] = row[1].get('title')\n",
    "    g_drs_series_type[drs_id] = row[1].get('table_series_type')\n",
    "    g_drs_series_num[drs_id] = row[1].get('table_series_num')\n",
    "    mlc = True\n",
    "    if (row[1].get('multilevel_columns') == 'No'):\n",
    "        mlc = False\n",
    "    mlr = True\n",
    "    if (row[1].get('multilevel_rows') == 'No'):\n",
    "        mlr = False\n",
    "    g_drs_multilevel_columns[drs_id] = mlc\n",
    "    g_drs_multilevel_rows[drs_id] = mlr\n",
    "    g_drs_computation_ready[drs_id] = row[1].get('computation_ready')\n",
    "    g_drs_tablegroup[drs_id] = row[1].get('table_group')\n",
    "    g_drs_tablegroup_members[drs_id] = row[1].get('table_group_members')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate the dataverse_inventory `DataFrame`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, populate `dataverse_inventory_df` with contents of the mapped inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate with contents of the mapped inventory\n",
    "for row in vendor_inventory_df.iterrows():\n",
    "    drs_id = row[1].get('drs_id')\n",
    "    url = ''\n",
    "    filename_osn = row[1].get('filename_osn')\n",
    "    filepath_osn = row[1].get('filepath_osn')\n",
    "    file_type = row[1].get('file_type')\n",
    "    table_title = None\n",
    "    table_type = None\n",
    "    multilevel_columns = None\n",
    "    multilevel_rows = None\n",
    "    computation_ready = False\n",
    "    table_group = False\n",
    "    table_group_members = []\n",
    "    shape = pd.NA\n",
    "    size = pd.NA\n",
    "    columns = []\n",
    "    series_type = None\n",
    "    series_num = None\n",
    "    series_name = None\n",
    "    image_handwriting = False\n",
    "    image_two_page = False\n",
    "    related_image = None\n",
    "    related_csv = None\n",
    "    related_txt = None\n",
    "    related_alto = None\n",
    "    entities = []\n",
    "\n",
    "    # add row to dataverse inventory\n",
    "    dataverse_inventory_df.loc[len(dataverse_inventory_df.index)] = [drs_id,url,filename_osn, filepath_osn, file_type,\n",
    "                                                                     table_title,table_type,multilevel_columns,multilevel_rows,computation_ready,\n",
    "                                                                     table_group,table_group_members,shape,size,columns,series_type,series_num,series_name,\n",
    "                                                                     image_handwriting,image_two_page,related_image,related_csv,related_txt,related_alto,\n",
    "                                                                     entities]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect information about the shape and size for each table. Also collect column information for non-multilevel column tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to find actual datafiels\n",
    "dir = '../data/trade_statistics_renamed/'\n",
    "\n",
    "g_csv_errors = {}\n",
    "\n",
    "# get just csv files from the file\n",
    "csv_df = dataverse_inventory_df.loc[dataverse_inventory_df['file_type'] == 'csv']\n",
    "for row in csv_df.iterrows():\n",
    "    drs_id = row[1].get('drs_id')\n",
    "    filepath = dir + row[1].get('filepath_osn')\n",
    "    g_csv_errors[filepath] = {}\n",
    "    g_drs_csv_columns[drs_id] = []\n",
    "    if (os.path.exists(filepath)):\n",
    "        try: \n",
    "            df = pd.read_csv(filepath,index_col=None)\n",
    "            g_drs_csv_size[drs_id] = df.size\n",
    "            g_drs_csv_shape[drs_id] = df.shape\n",
    "            if(g_drs_multilevel_columns.get(drs_id) == False):\n",
    "                g_drs_csv_columns[drs_id] =list(df.columns)\n",
    "            g_csv_errors[filepath]['status'] = True\n",
    "            g_csv_errors[filepath]['message'] = 'Success'\n",
    "        except:\n",
    "            g_csv_errors[filepath]['status'] = False\n",
    "            g_csv_errors[filepath]['message'] = 'Failed to read CSV'\n",
    "    else:\n",
    "            g_csv_errors[filepath]['status'] = False\n",
    "            g_csv_errors[filepath]['message'] = 'File Not Found'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, based upon `file_type`, populate row values with contents from dictionaries of information collected from each of the other `DataFrame`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse the rows, updating values\n",
    "for row in dataverse_inventory_df.iterrows():\n",
    "    # get row data\n",
    "    drs_id = row[1].get('drs_id')\n",
    "    file_type = row[1].get('file_type')\n",
    "\n",
    "    # set row variables\n",
    "    url = g_drs_urls.get(drs_id)\n",
    "    handwriting = g_drs_handwriting.get(drs_id)\n",
    "    two_page = g_drs_handwriting.get(drs_id)\n",
    "    related_image = g_drs_image_osn.get(drs_id)\n",
    "    related_csv = g_drs_csv_osn.get(drs_id)\n",
    "    related_txt = g_drs_txt_osn.get(drs_id)\n",
    "    related_alto = g_drs_alto_osn.get(drs_id)\n",
    "    table_type = g_drs_table_type.get(drs_id)\n",
    "    table_title = g_drs_table_title.get(drs_id)\n",
    "    multilevel_columns = g_drs_multilevel_columns.get(drs_id)\n",
    "    multilevel_rows = g_drs_multilevel_rows.get(drs_id)\n",
    "    computation_ready = g_drs_computation_ready.get(drs_id)\n",
    "    table_group = g_drs_tablegroup.get(drs_id)\n",
    "    table_group_members = g_drs_tablegroup_members.get(drs_id)\n",
    "    size = g_drs_csv_size.get(drs_id)\n",
    "    shape = g_drs_csv_shape.get(drs_id)\n",
    "    columns = g_drs_csv_columns.get(drs_id)\n",
    "    series_type = g_drs_series_type.get(drs_id)\n",
    "    series_num = g_drs_series_num.get(drs_id)\n",
    "    series_name = ''\n",
    "    if ((series_type is None) or \n",
    "        (series_type == '') or\n",
    "        (series_type is pd.NA) or\n",
    "        (series_num is None) or\n",
    "        (series_num == '') or\n",
    "        (series_num is pd.NA)):\n",
    "        series_name = 'Miscellaneous'\n",
    "    else:\n",
    "        series_name = '{}:{:d}'.format(series_type, int(series_num))\n",
    "\n",
    "    # update row\n",
    "    index = row[0]\n",
    "    dataverse_inventory_df.at[index,'url'] = url\n",
    "    dataverse_inventory_df.at[index,'image_handwriting'] = handwriting\n",
    "    dataverse_inventory_df.at[index,'image_two_page'] = two_page\n",
    "    dataverse_inventory_df.at[index,'related_image'] = related_image\n",
    "    dataverse_inventory_df.at[index,'related_txt'] = related_txt\n",
    "    dataverse_inventory_df.at[index,'related_csv'] = related_csv    \n",
    "    dataverse_inventory_df.at[index,'table_type'] = table_type\n",
    "    dataverse_inventory_df.at[index,'table_title'] = table_title\n",
    "    dataverse_inventory_df.at[index,'multilevel_columns'] = multilevel_columns  \n",
    "    dataverse_inventory_df.at[index,'multilevel_rows'] = multilevel_rows\n",
    "    dataverse_inventory_df.at[index,'computation_ready'] = computation_ready\n",
    "    dataverse_inventory_df.at[index,'table_group'] = table_group\n",
    "    dataverse_inventory_df.at[index,'table_group_members'] = table_group_members\n",
    "    dataverse_inventory_df.at[index,'size'] = size\n",
    "    dataverse_inventory_df.at[index,'shape'] = shape\n",
    "    dataverse_inventory_df.at[index,'columns'] = columns\n",
    "    dataverse_inventory_df.at[index,'series_type'] = series_type\n",
    "    dataverse_inventory_df.at[index,'series_num'] = series_num\n",
    "    dataverse_inventory_df.at[index,'series_name'] = series_name\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the NLP entities file: find the matching `filename_osn` from the `vendor_inventory_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for filename_osn\n",
    "nlp_entities_df['filename_osn'] = ''\n",
    "\n",
    "# for each nlp entity, find the corresponding filename_osn\n",
    "for row in nlp_entities_df.iterrows():\n",
    "    filename = row[1].get('filename')\n",
    "    df = vendor_inventory_df.loc[vendor_inventory_df['filename'] == filename]\n",
    "    if (df.empty == False):\n",
    "        df_index = df.index[0]\n",
    "        filename_osn = df.at[df_index,'filename_osn']\n",
    "        index = row[0]\n",
    "        nlp_entities_df.at[index,'filename_osn'] = filename_osn\n",
    "    else:\n",
    "        print('Warning: not found in vendor inventory: {}'.format(filename))\n",
    "\n",
    "# add the entities to the correct filename_osn in the dataverse_inventory\n",
    "for row in nlp_entities_df.iterrows():\n",
    "    filename_osn = row[1].get('filename_osn')\n",
    "    entities = row[1].get('entities')\n",
    "    # update matching rows in dataverse_inventory_df with entities\n",
    "    dataverse_inventory_df.loc[dataverse_inventory_df['filename_osn'] == filename_osn, 'entities'] = entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add bibliographic metadata to the `dataverse_inventory_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataverse_inventory_df['volume_title'] = g_volume_metadata.get('title')\n",
    "dataverse_inventory_df['attribution'] = g_volume_metadata.get('attribution')\n",
    "dataverse_inventory_df['author'] = g_volume_metadata.get('author')\n",
    "dataverse_inventory_df['published'] = g_volume_metadata.get('published')\n",
    "dataverse_inventory_df['notes'] = g_volume_metadata.get('notes')\n",
    "dataverse_inventory_df['subjects'] = ';'.join(g_volume_metadata.get('subjects'))\n",
    "dataverse_inventory_df['creation_date'] = g_volume_metadata.get('creation_date')\n",
    "dataverse_inventory_df['record_id'] = g_volume_metadata.get('record_id')\n",
    "dataverse_inventory_df['permalink'] = g_volume_metadata.get('permalink')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save `dataverse_inventory_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataverse_inventory_df.to_csv(g_output_file,index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End document.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e80866da39f614c41262712a96c603cec09e65c25ffba1b64ff6a9fa5a13fe2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
